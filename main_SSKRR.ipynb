{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy import stats\n",
    "from smt.sampling_methods import LHS as LHS_sampling\n",
    "import sklearn.preprocessing\n",
    "from lib import MultiIndex, MPJacn, JNodeWt\n",
    "import spgl1\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "from scipy.special import eval_legendre, factorial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import LaTeX\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "plt.rc('axes', labelsize=25)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = r\"\\usepackage{amsmath} \\usepackage{amssymb}\"\n",
    "plt.rcParams['font.family'] = 'Latin Modern Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LegendrePoly(x, order):\n",
    "    norm = np.sqrt(2 / (2*order + 1))\n",
    "    legendre_poly = eval_legendre(order, x)/norm\n",
    "    return legendre_poly\n",
    "\n",
    "def ProdLegendrePoly(x, indx):\n",
    "    N_LD, dimension = np.shape(x)\n",
    "    prod = np.ones((N_LD, 1))\n",
    "    for dim in range(dimension):\n",
    "        legendre_poly = LegendrePoly(x[:, dim], indx[dim])\n",
    "        prod = prod*legendre_poly.reshape((-1, 1))\n",
    "    return prod\n",
    "\n",
    "def compute_rho_torch_pyro(_xobs, _F, Pif, Pic, _nugget, _kernel):\n",
    "    # Xf points\n",
    "    Kf = _kernel.forward(_xobs[Pif, :]) + _nugget*torch.eye(sum(Pif))\n",
    "    _Ff = _F[Pif]\n",
    "\n",
    "    # Xc points\n",
    "    Kc = _kernel.forward(_xobs[Pic, :]) + _nugget*torch.eye(sum(Pic))\n",
    "    _Fc = _F[Pic]\n",
    "\n",
    "    # Compute rho\n",
    "    den = (_Fc.T @ torch.linalg.solve(Kc, _Fc))\n",
    "    num = (_Ff.T @ torch.linalg.solve(Kf, _Ff))\n",
    "    rho = 1 - den / num\n",
    "    if rho < 0 or rho > 1:\n",
    "        raise ValueError(f\"Error in rho: {rho}\")\n",
    "    return rho\n",
    "\n",
    "def compute_RMS_from_data(_x):\n",
    "    \"\"\"\n",
    "    Compute the root mean square distance from a set of positions.\n",
    "    \"\"\"\n",
    "    squared_distance = []\n",
    "    try: # d-dimension\n",
    "        n_obs, dimension = np.shape(_x)\n",
    "        for i in range(n_obs):\n",
    "            for j in range(i+1, n_obs):\n",
    "                dist = 0\n",
    "                for dim in range(dimension):\n",
    "                    dist = dist + (_x[i, dim] - _x[j, dim])**2\n",
    "                squared_distance.append(dist)\n",
    "\n",
    "        RMS = np.sqrt( (2 / (n_obs * (n_obs-1)))*np.sum(squared_distance) )\n",
    "        print(\"RMS computed from {} observations of dimension {}: {}\".format(n_obs, dimension, RMS))\n",
    "        return RMS\n",
    "    except: # 1-dimension\n",
    "        n_obs = np.size(_x)\n",
    "        dimension = 1\n",
    "        for i in range(n_obs):\n",
    "            for j in range(i+1, n_obs):\n",
    "                dist = 0\n",
    "                dist = dist + (_x[i] - _x[j])**2\n",
    "                squared_distance.append(dist)\n",
    "\n",
    "        RMS = np.sqrt( (2 / (n_obs * (n_obs-1)))*np.sum(squared_distance) )\n",
    "        print(\"RMS computed from {} observations of dimension {}: {}\".format(n_obs, dimension, RMS))\n",
    "        return RMS\n",
    "\n",
    "def create_pif_pic(num_obs, Nf, Nc, seed = None):\n",
    "    if seed != None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    else:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # Pif points\n",
    "    Pitot = np.full((num_obs), False)\n",
    "    Pitot[:Nf] = True\n",
    "    rng.shuffle(Pitot)\n",
    "    Pif = Pitot\n",
    "\n",
    "    # Pic points\n",
    "    Pitot = np.full((num_obs), False)\n",
    "    Pitot[rng.choice(np.where(Pif == True)[0], size = Nc, replace = False)] = True\n",
    "    Pic = Pitot\n",
    "\n",
    "    return Pif, Pic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ishigami test case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ishigami function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGT(_x, a, b):\n",
    "    x, y, z = _x[:, 0], _x[:, 1], _x[:, 2]\n",
    "    out = np.sin(x) + a*(np.sin(y))**2 + b*((z)**4)*np.sin(x)\n",
    "    return out\n",
    "\n",
    "performance_func = \"Ishigami_3D\"\n",
    "a, b = 7, 0.1\n",
    "analytical_mean = a*0.5\n",
    "analytical_variance = 0.5 + a**2/8 + b**(2)*np.pi**(8)/18 + b*np.pi**(4)/5\n",
    "\n",
    "# Parameters of the study\n",
    "number_of_seeds = 10\n",
    "number_of_observations_LD_list = [100]\n",
    "\n",
    "# Bounds of the input space\n",
    "bounds = np.array([[-np.pi, np.pi], [-np.pi, np.pi], [-np.pi, np.pi]])\n",
    "\n",
    "# Scaling the inputs\n",
    "scaler = sklearn.preprocessing.MinMaxScaler((-1, 1))\n",
    "scaler.fit(bounds.T)\n",
    "\n",
    "dimension = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate modeling methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSKRR Algorithm and sparse gPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters\n",
    "nuggets = 10**(np.linspace(np.log10(1e-12), np.log10(1e0), num = 31))\n",
    "\n",
    "# Save the predictions on the test dataset\n",
    "Y_SSKRR_TD_list = []\n",
    "Y_PCE_TD_list = []\n",
    "Y_TD_list = []\n",
    "\n",
    "# Total order of the basis\n",
    "total_order = 10\n",
    "\n",
    "# Compute the corresponding indexes\n",
    "indexes, _ = MultiIndex.MultiIndex(total_order, dimension)\n",
    "\n",
    "# Corresponds to uniform distribution\n",
    "alpha = [0]*dimension; beta = [0]*dimension\n",
    "\n",
    "for number_of_observations_LD in number_of_observations_LD_list:\n",
    "    for current_seed in range(number_of_seeds):\n",
    "        print(f\"Current seed: {current_seed}\")\n",
    "        seed = current_seed\n",
    "        number_of_observations = number_of_observations_LD\n",
    "\n",
    "        # Define the learning set\n",
    "        # Number of observatitons of the learning set\n",
    "        N_LD = number_of_observations\n",
    "\n",
    "        # Sampling the learning set\n",
    "        sampling = LHS_sampling(xlimits = bounds, criterion = \"maximin\", random_state = seed)\n",
    "        X_LD = sampling(N_LD)\n",
    "        X_LD_scaled = scaler.transform(X_LD)\n",
    "        Y_LD = FGT(X_LD, a, b)\n",
    "\n",
    "        # Define Kappa for the SSKRR algorithm\n",
    "        kappa_var = np.var(Y_LD)\n",
    "\n",
    "        # SPGL1 Optimization\n",
    "        # min |c|_L1 s. t. ||Tc - b||_2 <= epsilon\n",
    "\n",
    "        print(f\"Compute theta for SPGL1 using PCE with total order {total_order} corresponding to {len(indexes)} expansion coefficients\")\n",
    "        theta = np.zeros((N_LD, len(indexes)))\n",
    "        for obs in range(N_LD):\n",
    "            theta[obs, :] = MPJacn.MPJacn(X_LD_scaled[obs, :], total_order, alpha, beta, indexes)\n",
    "\n",
    "        # Convergence criterion for SPGL1\n",
    "        sigma = 1e-6\n",
    "        opt_tol = 1e-8\n",
    "\n",
    "        # SPGL1 solver\n",
    "        c, resid, grad, info = spgl1.spg_bpdn(theta, np.reshape(Y_LD, (-1,)), sigma, verbosity=3, iter_lim = int(1e5), opt_tol = opt_tol)\n",
    "\n",
    "        # Plot the SPGL1 solution of the expansion coefficients\n",
    "        plt.figure()\n",
    "        plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "        plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "        plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "        plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        # c_k values and number of basis functions\n",
    "        ck_value, R_S = c, np.shape(c)[0]\n",
    "\n",
    "        # Matrix corresponding to the values of the basis at the observations\n",
    "        e_sparse = theta\n",
    "\n",
    "        # PCE\n",
    "        # Norm for moment order of PCE\n",
    "        norm_moment_order = 1\n",
    "        for dim in range(dimension):\n",
    "            norm_moment_order = norm_moment_order*(2**(alpha[dim]+beta[dim]+1)*factorial(alpha[dim])*factorial(beta[dim])/factorial(alpha[dim]+beta[dim]+1))\n",
    "        norm_moment_order = np.sqrt(norm_moment_order)\n",
    "\n",
    "        # Mean using PCE\n",
    "        mean_PCE = ck_value[0] / norm_moment_order\n",
    "        # Variance using PCE\n",
    "        var_PCE = np.sum(ck_value[1:]**2) / norm_moment_order**2\n",
    "\n",
    "        # SSKRR Algorithm\n",
    "        # Minimize the eigenvalues\n",
    "        sigma_min = []\n",
    "        den = np.sum(np.abs(ck_value))\n",
    "        for k in range(R_S):\n",
    "            num = np.abs(ck_value[k])\n",
    "            sigma_temp = kappa_var*(num/den)\n",
    "            sigma_min.append(sigma_temp)\n",
    "\n",
    "        sigma_min = np.array(sigma_min).reshape((-1,))\n",
    "\n",
    "        # Test dataset\n",
    "        # Compute the error over test_sampling using N_TD sampling\n",
    "        N_TD = int(1e5)\n",
    "        np.random.seed(seed)\n",
    "        X_TD_scaled = np.random.uniform(-1, 1, size=(N_TD, dimension))\n",
    "        X_TD = scaler.inverse_transform(X_TD_scaled)\n",
    "        Y_TD = FGT(X_TD, a, b)\n",
    "        Y_TD_list.append(Y_TD)\n",
    "\n",
    "        # Compute PCE in the test dataset\n",
    "        e_PCE = np.zeros((N_TD, R_S))\n",
    "        for idx in range(R_S):\n",
    "            e_PCE[:, idx] = ProdLegendrePoly(X_TD_scaled, indexes[idx]).reshape((-1, ))\n",
    "        Y_PCE = e_PCE @ np.array(ck_value)\n",
    "        Y_PCE_TD_list.append(Y_PCE)\n",
    "\n",
    "        RMSE_gPC = np.sqrt(np.mean((Y_PCE - Y_TD)**2))\n",
    "\n",
    "        # Find the best value of the nugget through grid search\n",
    "        RMSE_optm_list = []\n",
    "        for nugget in nuggets:\n",
    "            # Prediction mean of the GP using the kernel given by the SSKRR algorithm\n",
    "            K_OBS = e_sparse @ np.diag(sigma_min) @ e_sparse.T\n",
    "            K_OBS = K_OBS + nugget*np.eye(N_LD)\n",
    "            K_PREDICT = e_PCE @ np.diag(sigma_min) @ e_sparse.T\n",
    "            Y_predict_optm = K_PREDICT @ np.linalg.solve(K_OBS, Y_LD)\n",
    "\n",
    "            # Save the current RMSE on the test set\n",
    "            RMSE_optm_list.append(np.sqrt(np.mean((Y_TD - Y_predict_optm)**2)))\n",
    "\n",
    "            # Only keep the best kernel\n",
    "            if (len(RMSE_optm_list) == 1) or (RMSE_optm_list[-1] < np.min(RMSE_optm_list[:-1])):\n",
    "                # Value of the minimum of the RMSE\n",
    "                min_RMSE_SSKRR = RMSE_optm_list[-1]\n",
    "\n",
    "                # Kappa corresponding to the minimum of the RMSE\n",
    "                min_nugget = nugget\n",
    "\n",
    "                # Best prediction corresponding to the minimum of the RMSE\n",
    "                Y_best_predict_optm = Y_predict_optm\n",
    "\n",
    "        Y_SSKRR_TD_list.append(Y_best_predict_optm)\n",
    "\n",
    "        print(f\"Log10(RMSE) with SSKRR algorithm: {np.log10(min_RMSE_SSKRR)}\")\n",
    "        print(f\"Log10(RMSE) with gPC: {np.log10(RMSE_gPC)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussion process regression surrogate using Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_nugget_kernel = 0\n",
    "Y_GPR_list = []\n",
    "for number_of_observations_LD in number_of_observations_LD_list:\n",
    "    for current_seed in range(number_of_seeds):\n",
    "        # Current seed for LHS\n",
    "        seed = current_seed\n",
    "\n",
    "        # Define the learning set\n",
    "        # Number of observatitons of the learning set\n",
    "        N_LD = number_of_observations_LD\n",
    "\n",
    "        # Sampling the learning set\n",
    "        sampling = LHS_sampling(xlimits = bounds, criterion = \"maximin\", random_state = seed)\n",
    "        X_LD = sampling(N_LD)\n",
    "        X_LD_scaled = scaler.transform(X_LD)\n",
    "        Y_LD = FGT(X_LD, a, b)\n",
    "\n",
    "        _, dimension = np.shape(X_LD)\n",
    "        kappa_var = np.var(Y_LD)\n",
    "\n",
    "        # Test dataset\n",
    "        # Compute the error over test_sampling using N_TD sampling\n",
    "        N_TD = int(1e5)\n",
    "        np.random.seed(seed)\n",
    "        X_TD_scaled = np.random.uniform(-1, 1, size=(N_TD, dimension))\n",
    "        X_TD = scaler.inverse_transform(X_TD_scaled)\n",
    "        Y_TD = FGT(X_TD, a, b)\n",
    "\n",
    "        # Initial lengthscale\n",
    "        # One lengthscale per input dimension\n",
    "        gamma = []\n",
    "        for d in range(dimension):\n",
    "            temp = compute_RMS_from_data(X_LD_scaled[:, d])/np.sqrt(2)\n",
    "            gamma.append(temp)\n",
    "\n",
    "        # GPR using pyro\n",
    "        Nf, Nc = (N_LD, int(N_LD/2))\n",
    "        ite_max = int(2000)\n",
    "        ite_save = int(100)\n",
    "\n",
    "        # Pif and Pic\n",
    "        Pif, Pic = create_pif_pic(N_LD, Nf, Nc, seed = None)\n",
    "\n",
    "        # Parameters of the algo\n",
    "        variable_gamma = torch.tensor(gamma, requires_grad=True)\n",
    "        nugget_optm = torch.tensor([init_nugget_kernel], requires_grad=False)\n",
    "        nugget_optm_list = []\n",
    "        nugget_optm_list.append(torch.clone(nugget_optm).detach().numpy())\n",
    "        variable_gamma_optm = [torch.clone(variable_gamma).detach().numpy()]\n",
    "\n",
    "        # Choice of the kernel\n",
    "        kernel = gp.kernels.RBF(input_dim = dimension, variance=torch.tensor(1.), lengthscale=variable_gamma) # RBF\n",
    "\n",
    "        # Compute the loss and choose the optimizer\n",
    "        loss = compute_rho_torch_pyro(torch.from_numpy(X_LD_scaled), torch.tensor(Y_LD), Pif, Pic, nugget_optm, kernel)\n",
    "        optimizer = torch.optim.Adam([kernel.lengthscale_unconstrained, nugget_optm], lr = 3e-4)\n",
    "\n",
    "        # Clear pyro parameters for new loop\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # GPR Prediction\n",
    "        gpr_opt = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "        value_torch, _ = gpr_opt(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "        value_torch = torch.clone(value_torch).detach().numpy()\n",
    "        RMSE_torch = np.sqrt(np.mean((Y_TD - value_torch)**2))\n",
    "\n",
    "        rho_optm = [loss]\n",
    "        print(f\"rho_optm: {rho_optm}\")\n",
    "        ite_list = [0]\n",
    "        RMSE_torch_list_temp = [RMSE_torch]\n",
    "        for k in range(ite_max):\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_rho_torch_pyro(torch.from_numpy(X_LD_scaled), torch.tensor(Y_LD), Pif, Pic, nugget_optm, kernel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                kernel.lengthscale[:] = kernel.lengthscale.clamp(0, 1e6)\n",
    "                nugget_optm[:] = nugget_optm.clamp(0, 1e6)\n",
    "                if (k+1)%ite_save == 0:\n",
    "                    print(f\"Learning: Iteration n. {k+1} / {ite_max}\")\n",
    "\n",
    "                    # Print current length scales of the kernel\n",
    "                    print(kernel.lengthscale)\n",
    "\n",
    "                    # Store loss value and current optimized parameters\n",
    "                    rho_optm.append(loss.item())\n",
    "                    ite_list.append((k+1))\n",
    "                    variable_gamma_optm.append(torch.clone(kernel.lengthscale).detach().numpy())\n",
    "                    nugget_optm_list.append(torch.clone(nugget_optm).detach().numpy())\n",
    "\n",
    "                    # Compute the test error\n",
    "                    gpr_opt = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "                    value_torch, _ = gpr_opt(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "                    value_torch = torch.clone(value_torch).detach().numpy()\n",
    "                    RMSE_torch = np.sqrt(np.mean((Y_TD - value_torch)**2))\n",
    "                    RMSE_torch_list_temp.append(RMSE_torch)\n",
    "\n",
    "            # Update new batch\n",
    "            Pif, Pic = create_pif_pic(N_LD, Nf, Nc, seed = None)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, RMSE_torch_list_temp, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\log_{10}(e_{RMSE})$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, rho_optm, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\rho$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, nugget_optm_list, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\lambda$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        # Compute with the best hyperparameters on the test set\n",
    "        idx_min_RMSE = np.argmin(RMSE_torch_list_temp)\n",
    "        gamma_min = variable_gamma_optm[idx_min_RMSE]\n",
    "        kernel_best = gp.kernels.RBF(input_dim = dimension, variance=torch.tensor(1.), lengthscale=torch.from_numpy(gamma_min)) # RBF\n",
    "        gpr_opt_best = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel_best, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "        value_torch_best, _ = gpr_opt_best(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "        value_torch_best = torch.clone(value_torch_best).detach().numpy()\n",
    "\n",
    "        Y_GPR_list.append(value_torch_best)\n",
    "\n",
    "        print(f\"Log10(RMSE): {np.log10(RMSE_torch_list_temp[idx_min_RMSE])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully tensorized gPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_FT_PCE_list = []\n",
    "\n",
    "for number_of_observations_LD in number_of_observations_LD_list:\n",
    "    # Define the learning set\n",
    "    # Number of observatitons of the learning set\n",
    "    N_LD = number_of_observations_LD\n",
    "\n",
    "    # Bounds of the input space\n",
    "    bounds = np.array([[-np.pi, np.pi], [-np.pi, np.pi], [-np.pi, np.pi]])\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler((-1, 1))\n",
    "    scaler.fit(bounds.T)\n",
    "\n",
    "    # Total order of the basis\n",
    "    total_order = 10\n",
    "\n",
    "    # Compute the corresponding indexes\n",
    "    indexes, _ = MultiIndex.MultiIndex(total_order, dimension)\n",
    "\n",
    "    # Corresponds to uniform distribution\n",
    "    alpha = [0, 0, 0]; beta = [0, 0, 0]\n",
    "\n",
    "    N_NODES = int(np.around((total_order*2 + 3) / 2))\n",
    "    CHOICE_QUAD = 2 # Correspond to GJL quadrature\n",
    "    N_LD = N_NODES**dimension\n",
    "    X_LD_scaled = np.zeros((N_NODES**dimension, dimension))\n",
    "    quad, weights = JNodeWt.JNodeWt(N_NODES-1, CHOICE_QUAD,0 ,0)\n",
    "    multi_dim_weights = np.zeros((N_NODES**dimension, dimension))\n",
    "    count = 0\n",
    "    for i in range(N_NODES):\n",
    "        for j in range(N_NODES):\n",
    "            for k in range(N_NODES):\n",
    "                X_LD_scaled[count, :] = quad[i], quad[j], quad[k]\n",
    "                multi_dim_weights[count, :] = weights[i], weights[j], weights[k]\n",
    "                count = count + 1\n",
    "    prod_multi_dim_weights = np.prod(multi_dim_weights, axis = 1)\n",
    "    diag_prod_multi_dim_weights = np.diag(prod_multi_dim_weights)\n",
    "    X_LD = scaler.inverse_transform(X_LD_scaled)\n",
    "    Y_LD = FGT(X_LD, a, b)\n",
    "\n",
    "    indexes, _ = MultiIndex.MultiIndex(total_order, dimension)\n",
    "    theta = np.zeros((N_LD, len(indexes)))\n",
    "    alpha = [0, 0, 0]; beta = [0, 0, 0] # Corresponds to uniform distribution\n",
    "    print(f\"Compute theta vector using PCE with total order {total_order} corresponding to {len(indexes)} parameters\")\n",
    "    for obs in range(N_LD):\n",
    "        theta[obs, :] = MPJacn.MPJacn(X_LD_scaled[obs, :], total_order, alpha, beta, indexes)\n",
    "\n",
    "    # Compute the expansion coefficients size (P+1, 1)\n",
    "    c = Y_LD.T @ diag_prod_multi_dim_weights @ theta\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "    plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "    plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "    plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "    plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "    plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    # c_k values and number of basis functions\n",
    "    ck_value, R_S = c, np.shape(c)[0]\n",
    "\n",
    "    # Matrix corresponding to the values of the basis at the observations\n",
    "    e_sparse = theta\n",
    "\n",
    "    for current_seed in range(number_of_seeds):\n",
    "        seed = current_seed\n",
    "        # Test dataset\n",
    "        # Compute the error over test_sampling using N_TD sampling\n",
    "        N_TD = int(1e5)\n",
    "        np.random.seed(seed)\n",
    "        X_TD_scaled = np.random.uniform(-1, 1, size=(N_TD, dimension))\n",
    "        X_TD = scaler.inverse_transform(X_TD_scaled)\n",
    "        Y_TD = FGT(X_TD, a, b)\n",
    "        # Compute PCE in the test dataset\n",
    "        e_PCE = np.zeros((N_TD, R_S))\n",
    "        for idx in range(R_S):\n",
    "            e_PCE[:, idx] = ProdLegendrePoly(X_TD_scaled, indexes[idx]).reshape((-1, ))\n",
    "        Y_PCE = e_PCE @ np.array(ck_value)\n",
    "        Y_FT_PCE_list.append(Y_PCE)\n",
    "        RMSE_FT_gPC = np.sqrt(np.mean((Y_PCE - Y_TD)**2))\n",
    "\n",
    "        print(f\"Log10(RMSE) with gPC: {np.log10(RMSE_FT_gPC)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRMSE_optm_list, RMSE_optm_list = [], []\n",
    "NRMSE_PCE_list, RMSE_PCE_list = [], []\n",
    "NRMSE_RBF_list, RMSE_RBF_list = [], []\n",
    "NRMSE_FullT_list, RMSE_FullT_list = [], []\n",
    "\n",
    "for k in range(number_of_seeds):\n",
    "        Y_TD_current = Y_TD_list[k]\n",
    "        Y_PCE_current = Y_PCE_TD_list[k]\n",
    "        Y_GPR_current = Y_GPR_list[k]\n",
    "        Y_FT_PCE_current = Y_FT_PCE_list[k]\n",
    "        Y_SSKRR_current = Y_SSKRR_TD_list[k]\n",
    "\n",
    "        # NRMSE\n",
    "        NRMSE_PCE_list.append(np.sqrt(np.sum((Y_TD_current - Y_PCE_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "        NRMSE_optm_list.append(np.sqrt(np.sum((Y_TD_current - Y_SSKRR_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "        NRMSE_RBF_list.append(np.sqrt(np.sum((Y_TD_current - Y_GPR_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "        NRMSE_FullT_list.append(np.sqrt(np.sum((Y_TD_current - Y_FT_PCE_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "\n",
    "        # RMSE\n",
    "        RMSE_PCE_list.append(np.sqrt(np.mean((Y_TD_current - Y_PCE_current)**2)))\n",
    "        RMSE_optm_list.append(np.sqrt(np.mean((Y_TD_current - Y_SSKRR_current)**2)))\n",
    "        RMSE_RBF_list.append(np.sqrt(np.mean((Y_TD_current - Y_GPR_current)**2)))\n",
    "        RMSE_FullT_list.append(np.sqrt(np.mean((Y_TD_current - Y_FT_PCE_current)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "plt.figure()\n",
    "plt.boxplot([np.log10(NRMSE_optm_list), np.log10(NRMSE_PCE_list), np.log10(NRMSE_FullT_list), np.log10(NRMSE_RBF_list)])\n",
    "plt.xticks([1, 2, 3, 4], [\"SSKRR\", \"Sparse gPC\", \"Fully tensorized gPC\", \"KRR\"])\n",
    "plt.grid(True, color = \"black\", which=\"major\", ls=\"-.\", linewidth = 0.5)\n",
    "plt.ylabel(r\"$\\log_{10}(e_{\\mathrm{NRMSE}})$\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot([np.log10(RMSE_optm_list), np.log10(RMSE_PCE_list), np.log10(RMSE_FullT_list), np.log10(RMSE_RBF_list)])\n",
    "plt.xticks([1, 2, 3, 4], [\"SSKRR\", \"Sparse gPC\", \"Fully tensorized gPC\", \"KRR\"])\n",
    "plt.grid(True, color = \"black\", which=\"major\", ls=\"-.\", linewidth = 0.5)\n",
    "plt.ylabel(r\"$\\log_{10}(e_{\\mathrm{RMSE}})$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TD_current = Y_TD_list[0]\n",
    "Y_PCE_current = Y_PCE_TD_list[0]\n",
    "Y_GPR_current = Y_GPR_list[0]\n",
    "Y_FT_PCE_current = Y_FT_PCE_list[0]\n",
    "Y_SSKRR_current = Y_SSKRR_TD_list[0]\n",
    "\n",
    "# Ground Truth\n",
    "GT_data = Y_TD_current\n",
    "test_GT = np.linspace(np.min(GT_data), np.max(GT_data), num = int(1e5))\n",
    "pdf_predict_GT = stats.gaussian_kde(GT_data)(test_GT)\n",
    "pdf_predict_GT_sorted = np.sort(pdf_predict_GT)\n",
    "\n",
    "# Sparse gPC\n",
    "gPC_data = Y_PCE_current\n",
    "pdf_predict_PCE = stats.gaussian_kde(gPC_data)(test_GT)\n",
    "pdf_predict_PCE_sorted = np.sort(pdf_predict_PCE)\n",
    "\n",
    "# SSKRR\n",
    "optm_data = Y_SSKRR_current\n",
    "pdf_predict_optm = stats.gaussian_kde(optm_data)(test_GT)\n",
    "pdf_predict_optm_sorted = np.sort(pdf_predict_optm)\n",
    "\n",
    "# KRR with Gaussian Kernel\n",
    "RBF_data = Y_GPR_current\n",
    "pdf_predict_RBF = stats.gaussian_kde(RBF_data)(test_GT)\n",
    "pdf_predict_RBF_sorted = np.sort(pdf_predict_RBF)\n",
    "\n",
    "# Full tensorized gPC\n",
    "FullT_data = Y_FT_PCE_current\n",
    "pdf_predict_FullT = stats.gaussian_kde(FullT_data)(test_GT)\n",
    "pdf_predict_FullT_sorted = np.sort(pdf_predict_FullT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDF\n",
    "fig = plt.figure()\n",
    "plt.plot(test_GT, pdf_predict_optm, 'rD-.', linewidth=0.5, alpha = 1, label = \"SSKRR\", markevery = 1000, markersize = 1.5)\n",
    "plt.plot(test_GT, pdf_predict_PCE, 'b*-.', linewidth=0.5, alpha = 1, label = \" Sparse gPC\", markevery = 1000, markersize = 1.5)\n",
    "plt.plot(test_GT, pdf_predict_FullT, 'c^-.', linewidth=0.5, alpha = 1, label = \"Fully tensorized gPC\", markevery = 1000, markersize = 1.5)\n",
    "plt.plot(test_GT, pdf_predict_RBF, 'ms-.', linewidth=0.5, alpha = 1, label = \"KRR\", markevery = 1000, markersize = 1.5)\n",
    "plt.plot(test_GT, pdf_predict_GT, 'k', linestyle = 'dotted', linewidth=0.5, alpha = 1, label = \"Ground truth\")\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid(True, color = \"black\", which=\"major\", ls=\"-.\", linewidth = 0.20)\n",
    "plt.legend(fontsize = 9, loc=\"upper right\")\n",
    "\n",
    "# Create zoom-out plot\n",
    "ax_new = fig.add_axes([0.18, 0.65, 0.2, 0.2]) # the position of zoom-out plot compare to the ratio of zoom-in plot\n",
    "ax_new.plot(test_GT, pdf_predict_optm, 'rD-.', linewidth=0.5, alpha = 1, label = \"SSKRR\", markevery = 1000, markersize = 1.5)\n",
    "ax_new.plot(test_GT, pdf_predict_PCE, 'b*-.', linewidth=0.5, alpha = 1, label = \" Sparse gPC\", markevery = 1000, markersize = 1.5)\n",
    "ax_new.plot(test_GT, pdf_predict_FullT, 'c^-.', linewidth=0.5, alpha = 1, label = \"Fully tensorized gPC\", markevery = 1000, markersize = 1.5)\n",
    "ax_new.plot(test_GT, pdf_predict_RBF, 'ms-.', linewidth=0.5, alpha = 1, label = \"KRR\", markevery = 1000, markersize = 1.5)\n",
    "ax_new.plot(test_GT, pdf_predict_GT, 'k', linestyle = 'dotted', linewidth=0.5, alpha = 1, label = \"Ground truth\")\n",
    "ax_new.set_xlim(3, 7)\n",
    "ax_new.set_ylim(0.081, 0.11)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenbrock test function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenbrock function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(feature_range=(-1, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FGT(_x):\n",
    "    dimension = np.shape(_x)[1]\n",
    "    rosenbrock = 0\n",
    "    for d in range(dimension-1):\n",
    "        rosenbrock = rosenbrock + (100*(_x[:, d+1] - _x[:, d]**2)**2 + (1-_x[:, d])**2)\n",
    "    return rosenbrock\n",
    "\n",
    "performance_func = \"Rosenbroke_10D\"\n",
    "\n",
    "# Parameters of the study\n",
    "number_of_seeds = 10\n",
    "number_of_observations_LD_list = [400]\n",
    "\n",
    "# Bounds of the input space\n",
    "dimension = 10\n",
    "bounds = np.zeros((dimension, 2))\n",
    "bounds[:, 0], bounds[:, 1] = -2, 2\n",
    "# Scaling the inputs\n",
    "scaler = sklearn.preprocessing.MinMaxScaler((-1, 1))\n",
    "scaler.fit(bounds.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate modeling methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSKRR Algorithm and sparse gPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters\n",
    "nuggets = 10**(np.linspace(np.log10(1e-12), np.log10(1e0), num = 31))\n",
    "\n",
    "# Save the predictions on the test dataset\n",
    "Y_SSKRR_TD_list = []\n",
    "Y_PCE_TD_list = []\n",
    "Y_TD_list = []\n",
    "\n",
    "# Total order of the basis\n",
    "total_order = 4\n",
    "\n",
    "# Compute the corresponding indexes\n",
    "indexes, _ = MultiIndex.MultiIndex(total_order, dimension)\n",
    "\n",
    "# Corresponds to uniform distribution\n",
    "alpha = [0]*dimension; beta = [0]*dimension\n",
    "\n",
    "for number_of_observations_LD in number_of_observations_LD_list:\n",
    "    for current_seed in range(number_of_seeds):\n",
    "        print(f\"Current seed: {current_seed}\")\n",
    "        seed = current_seed\n",
    "        number_of_observations = number_of_observations_LD\n",
    "\n",
    "        # Define the learning set\n",
    "        # Number of observatitons of the learning set\n",
    "        N_LD = number_of_observations\n",
    "\n",
    "        # Sampling the learning set\n",
    "        sampling = LHS_sampling(xlimits = bounds, criterion = \"maximin\", random_state = seed)\n",
    "        X_LD = sampling(N_LD)\n",
    "        X_LD_scaled = scaler.transform(X_LD)\n",
    "        Y_LD = FGT(X_LD)\n",
    "\n",
    "        _, dimension = np.shape(X_LD)\n",
    "        kappa_var = np.var(Y_LD)\n",
    "\n",
    "        # SPGL1 Optimization\n",
    "        # min |c|_L1 s. t. ||Tc - b||_2 <= epsilon\n",
    "\n",
    "        print(f\"Compute theta for SPGL1 using PCE with total order {total_order} corresponding to {len(indexes)} expansion coefficients\")\n",
    "        theta = np.zeros((N_LD, len(indexes)))\n",
    "        for obs in range(N_LD):\n",
    "            theta[obs, :] = MPJacn.MPJacn(X_LD_scaled[obs, :], total_order, alpha, beta, indexes)\n",
    "\n",
    "        # Convergence criterion for SPGL1\n",
    "        sigma = 1e-6\n",
    "        opt_tol = 1e-7\n",
    "\n",
    "        # SPGL1 solver\n",
    "        c, resid, grad, info = spgl1.spg_bpdn(theta, np.reshape(Y_LD, (-1,)), sigma, verbosity=3, iter_lim = int(1e5), opt_tol = opt_tol)\n",
    "\n",
    "        # Plot the SPGL1 solution of the expansion coefficients\n",
    "        plt.figure()\n",
    "        plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "        plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "        plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(abs(c), 'or', markersize=3, mec='k', mew = 0.5)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(r\"i--th Legendre polynomial\"); plt.ylabel(r\"$|c_i|$\")\n",
    "        plt.grid(True, color = \"black\", which=\"major\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.grid(True, color = \"gray\", which=\"minor\", ls=\"--\", linewidth = 0.25)\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        # c_k values and number of basis functions\n",
    "        ck_value, R_S = c, np.shape(c)[0]\n",
    "\n",
    "        # Matrix corresponding to the values of the basis at the observations\n",
    "        e_sparse = theta\n",
    "\n",
    "        # PCE\n",
    "        # Norm for moment order of PCE\n",
    "        norm_moment_order = 1\n",
    "        for dim in range(dimension):\n",
    "            norm_moment_order = norm_moment_order*(2**(alpha[dim]+beta[dim]+1)*factorial(alpha[dim])*factorial(beta[dim])/factorial(alpha[dim]+beta[dim]+1))\n",
    "        norm_moment_order = np.sqrt(norm_moment_order)\n",
    "\n",
    "        # Mean using PCE\n",
    "        mean_PCE = ck_value[0] / norm_moment_order\n",
    "        # Variance using PCE\n",
    "        var_PCE = np.sum(ck_value[1:]**2) / norm_moment_order**2\n",
    "\n",
    "        # SSKRR Algorithm\n",
    "        # Minimize the eigenvalues\n",
    "        sigma_min = []\n",
    "        den = np.sum(np.abs(ck_value))\n",
    "        for k in range(R_S):\n",
    "            num = np.abs(ck_value[k])\n",
    "            sigma_temp = kappa_var*(num/den)\n",
    "            sigma_min.append(sigma_temp)\n",
    "\n",
    "        sigma_min = np.array(sigma_min).reshape((-1,))\n",
    "\n",
    "        # Test dataset\n",
    "        # Compute the error over test_sampling using N_TD sampling\n",
    "        N_TD = int(1e5)\n",
    "        np.random.seed(seed)\n",
    "        X_TD_scaled = np.random.uniform(-1, 1, size=(N_TD, dimension))\n",
    "        X_TD = scaler.inverse_transform(X_TD_scaled)\n",
    "        Y_TD = FGT(X_TD)\n",
    "        Y_TD_list.append(Y_TD)\n",
    "\n",
    "        # Compute PCE in the test dataset\n",
    "        e_PCE = np.zeros((N_TD, R_S))\n",
    "        for idx in range(R_S):\n",
    "            e_PCE[:, idx] = ProdLegendrePoly(X_TD_scaled, indexes[idx]).reshape((-1, ))\n",
    "        Y_PCE = e_PCE @ np.array(ck_value)\n",
    "        Y_PCE_TD_list.append(Y_PCE)\n",
    "\n",
    "        RMSE_gPC = np.sqrt(np.mean((Y_PCE - Y_TD)**2))\n",
    "\n",
    "        # Find the best value of the nugget through grid search\n",
    "        RMSE_optm_list = []\n",
    "        for nugget in nuggets:\n",
    "            # Prediction mean of the GP using the kernel given by the SSKRR algorithm\n",
    "            K_OBS = e_sparse @ np.diag(sigma_min) @ e_sparse.T\n",
    "            K_OBS = K_OBS + nugget*np.eye(N_LD)\n",
    "            K_PREDICT = e_PCE @ np.diag(sigma_min) @ e_sparse.T\n",
    "            Y_predict_optm = K_PREDICT @ np.linalg.solve(K_OBS, Y_LD)\n",
    "\n",
    "            # Save the current RMSE on the test set\n",
    "            RMSE_optm_list.append(np.sqrt(np.mean((Y_TD - Y_predict_optm)**2)))\n",
    "\n",
    "            # Only keep the best kernel\n",
    "            if (len(RMSE_optm_list) == 1) or (RMSE_optm_list[-1] < np.min(RMSE_optm_list[:-1])):\n",
    "                # Value of the minimum of the RMSE\n",
    "                min_RMSE_SSKRR = RMSE_optm_list[-1]\n",
    "\n",
    "                # Kappa corresponding to the minimum of the RMSE\n",
    "                min_nugget = nugget\n",
    "\n",
    "                # Best prediction corresponding to the minimum of the RMSE\n",
    "                Y_best_predict_optm = Y_predict_optm\n",
    "\n",
    "        Y_SSKRR_TD_list.append(Y_best_predict_optm)\n",
    "\n",
    "        print(f\"Log10(RMSE) with SSKRR algorithm: {np.log10(min_RMSE_SSKRR)}\")\n",
    "        print(f\"Log10(RMSE) with gPC: {np.log10(RMSE_gPC)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian process regression with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_nugget_kernel = 0\n",
    "Y_GPR_list = []\n",
    "for number_of_observations_LD in number_of_observations_LD_list:\n",
    "    for current_seed in range(number_of_seeds):\n",
    "        # Current seed for LHS\n",
    "        seed = current_seed\n",
    "\n",
    "        # Define the learning set\n",
    "        # Number of observatitons of the learning set\n",
    "        N_LD = number_of_observations_LD\n",
    "\n",
    "        # Sampling the learning set\n",
    "        sampling = LHS_sampling(xlimits = bounds, criterion = \"maximin\", random_state = seed)\n",
    "        X_LD = sampling(N_LD)\n",
    "        X_LD_scaled = scaler.transform(X_LD)\n",
    "        Y_LD = FGT(X_LD)\n",
    "\n",
    "        _, dimension = np.shape(X_LD)\n",
    "        kappa_var = np.var(Y_LD)\n",
    "        # Test dataset\n",
    "        # Compute the error over test_sampling using N_TD sampling\n",
    "        N_TD = int(1e5)\n",
    "        np.random.seed(seed)\n",
    "        X_TD_scaled = np.random.uniform(-1, 1, size=(N_TD, dimension))\n",
    "        X_TD = scaler.inverse_transform(X_TD_scaled)\n",
    "        Y_TD = FGT(X_TD)\n",
    "\n",
    "        # Initial lengthscale\n",
    "        # One lengthscale per input dimension\n",
    "        gamma = []\n",
    "        for d in range(dimension):\n",
    "            temp = compute_RMS_from_data(X_LD_scaled[:, d])/np.sqrt(2)\n",
    "            gamma.append(temp)\n",
    "\n",
    "        # GPR using pyro\n",
    "        Nf, Nc = (N_LD, int(N_LD/2))\n",
    "        ite_max = int(10000)\n",
    "        ite_save = int(100)\n",
    "\n",
    "        # Pif and Pic\n",
    "        Pif, Pic = create_pif_pic(N_LD, Nf, Nc, seed = None)\n",
    "\n",
    "        # Parameters of the algo\n",
    "        variable_gamma = torch.tensor(gamma, requires_grad=True)\n",
    "        nugget_optm = torch.tensor([init_nugget_kernel], requires_grad=False)\n",
    "        nugget_optm_list = []\n",
    "        nugget_optm_list.append(torch.clone(nugget_optm).detach().numpy())\n",
    "        variable_gamma_optm = [torch.clone(variable_gamma).detach().numpy()]\n",
    "\n",
    "        # Choice of the kernel\n",
    "        kernel = gp.kernels.RBF(input_dim = dimension, variance=torch.tensor(1.), lengthscale=variable_gamma) # RBF\n",
    "\n",
    "        # Compute the loss and choose the optimizer\n",
    "        loss = compute_rho_torch_pyro(torch.from_numpy(X_LD_scaled), torch.tensor(Y_LD), Pif, Pic, nugget_optm, kernel)\n",
    "        optimizer = torch.optim.Adam([kernel.lengthscale_unconstrained, nugget_optm], lr = 3e-4)\n",
    "\n",
    "        # Clear pyro parameters for new loop\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # GPR Prediction\n",
    "        gpr_opt = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "        value_torch, _ = gpr_opt(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "        value_torch = torch.clone(value_torch).detach().numpy()\n",
    "        RMSE_torch = np.sqrt(np.mean((Y_TD - value_torch)**2))\n",
    "        rho_optm = [loss]\n",
    "        print(f\"rho_optm: {rho_optm}\")\n",
    "        ite_list = [0]\n",
    "        RMSE_torch_list_temp = [RMSE_torch]\n",
    "        for k in range(ite_max):\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_rho_torch_pyro(torch.from_numpy(X_LD_scaled), torch.tensor(Y_LD), Pif, Pic, nugget_optm, kernel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                kernel.lengthscale[:] = kernel.lengthscale.clamp(0, 1e6)\n",
    "                nugget_optm[:] = nugget_optm.clamp(0, 1e6)\n",
    "                if (k+1)%ite_save == 0:\n",
    "                    print(f\"Learning: Iteration n. {k+1} / {ite_max}\")\n",
    "\n",
    "                    # Print current length scales of the kernel\n",
    "                    print(kernel.lengthscale)\n",
    "\n",
    "                    # Store loss value and current optimized parameters\n",
    "                    rho_optm.append(loss.item())\n",
    "                    ite_list.append((k+1))\n",
    "                    variable_gamma_optm.append(torch.clone(kernel.lengthscale).detach().numpy())\n",
    "                    nugget_optm_list.append(torch.clone(nugget_optm).detach().numpy())\n",
    "\n",
    "                    # Compute the test error\n",
    "                    gpr_opt = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "                    value_torch, _ = gpr_opt(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "                    value_torch = torch.clone(value_torch).detach().numpy()\n",
    "                    RMSE_torch = np.sqrt(np.mean((Y_TD - value_torch)**2))\n",
    "                    RMSE_torch_list_temp.append(RMSE_torch)\n",
    "\n",
    "            # Update new batch\n",
    "            Pif, Pic = create_pif_pic(N_LD, Nf, Nc, seed = None)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, RMSE_torch_list_temp, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\log_{10}(e_{RMSE})$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, rho_optm, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\rho$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(ite_list, nugget_optm_list, '-ok', markersize = 4, markeredgecolor = 'r', markeredgewidth = 1)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(r\"Number of iterations\")\n",
    "        plt.ylabel(r\"$\\lambda$\")\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        # Compute with the best hyperparameters on the test set\n",
    "        idx_min_RMSE = np.argmin(RMSE_torch_list_temp)\n",
    "        gamma_min = variable_gamma_optm[idx_min_RMSE]\n",
    "        kernel_best = gp.kernels.RBF(input_dim = dimension, variance=torch.tensor(1.), lengthscale=torch.from_numpy(gamma_min)) # RBF\n",
    "        gpr_opt_best = gp.models.GPRegression(torch.from_numpy(X_LD_scaled), torch.from_numpy(Y_LD), kernel_best, noise=torch.tensor(0.), jitter = nugget_optm)\n",
    "        value_torch_best, _ = gpr_opt_best(torch.from_numpy(X_TD_scaled), full_cov=False, noiseless=True)\n",
    "        value_torch_best = torch.clone(value_torch_best).detach().numpy()\n",
    "\n",
    "        Y_GPR_list.append(value_torch_best)\n",
    "\n",
    "        print(f\"Log10(RMSE): {np.log10(RMSE_torch_list_temp[idx_min_RMSE])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRMSE_optm_list, RMSE_optm_list = [], []\n",
    "NRMSE_PCE_list, RMSE_PCE_list = [], []\n",
    "NRMSE_RBF_list, RMSE_RBF_list, = [], []\n",
    "\n",
    "for k in range(number_of_seeds):\n",
    "        Y_TD_current = Y_TD_list[k]\n",
    "        Y_PCE_current = Y_PCE_TD_list[k]\n",
    "        Y_GPR_current = Y_GPR_list[k]\n",
    "        Y_SSKRR_current = Y_SSKRR_TD_list[k]\n",
    "\n",
    "        # NRMSE\n",
    "        NRMSE_PCE_list.append(np.sqrt(np.sum((Y_TD_current - Y_PCE_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "        NRMSE_optm_list.append(np.sqrt(np.sum((Y_TD_current - Y_SSKRR_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "        NRMSE_RBF_list.append(np.sqrt(np.sum((Y_TD_current - Y_GPR_current)**2) / np.sum((Y_TD_current)**2)))\n",
    "\n",
    "        # RMSE\n",
    "        RMSE_PCE_list.append(np.sqrt(np.mean((Y_TD_current - Y_PCE_current)**2)))\n",
    "        RMSE_optm_list.append(np.sqrt(np.mean((Y_TD_current - Y_SSKRR_current)**2)))\n",
    "        RMSE_RBF_list.append(np.sqrt(np.mean((Y_TD_current - Y_GPR_current)**2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TD_current = Y_TD_list[0]\n",
    "Y_PCE_current = Y_PCE_TD_list[0]\n",
    "Y_GPR_current = Y_GPR_list[0]\n",
    "Y_SSKRR_current = Y_SSKRR_TD_list[0]\n",
    "\n",
    "# Ground Truth\n",
    "GT_data = Y_TD_current\n",
    "test_GT = np.linspace(np.min(GT_data), np.max(GT_data), num = int(1e5))\n",
    "pdf_predict_GT = stats.gaussian_kde(GT_data)(test_GT)\n",
    "pdf_predict_GT_sorted = np.sort(pdf_predict_GT)\n",
    "\n",
    "# Sparse gPC\n",
    "gPC_data = Y_PCE_current\n",
    "pdf_predict_PCE = stats.gaussian_kde(gPC_data)(test_GT)\n",
    "pdf_predict_PCE_sorted = np.sort(pdf_predict_PCE)\n",
    "\n",
    "# SSKRR\n",
    "optm_data = Y_SSKRR_current\n",
    "pdf_predict_optm = stats.gaussian_kde(optm_data)(test_GT)\n",
    "pdf_predict_optm_sorted = np.sort(pdf_predict_optm)\n",
    "\n",
    "# KRR with Gaussian Kernel\n",
    "RBF_data = Y_GPR_current\n",
    "pdf_predict_RBF = stats.gaussian_kde(RBF_data)(test_GT)\n",
    "pdf_predict_RBF_sorted = np.sort(pdf_predict_RBF)\n",
    "\n",
    "# Full tensorized gPC\n",
    "FullT_data = Y_FT_PCE_current\n",
    "pdf_predict_FullT = stats.gaussian_kde(FullT_data)(test_GT)\n",
    "pdf_predict_FullT_sorted = np.sort(pdf_predict_FullT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
